Assignment 4
=============

Problem 20
----------
PCA is implemented in the file PCA.py

Problem 21
----------
PCA scatter plots are saved in Observations/PCA/2015-05-28 22:38:13

Problem 22
-----------
Sparse auto encoder is implemented in the file SparseAutoEncoder.py. Training using squared error in the folder : Observations/AutoEncoder/2015-05-31 23:58:30

Problem 23
-----------
Training using l1 penalty and squared error can be found in the folder. We can see that more hidden units are forced towards zero when we add the penalty term



Problem 24
-----------

Sparsity condition forces most of the hidden unit activations to stay close to the zero value. 

As sparsity hyperparmeter increases the reconstructions become less accurate as more hidden units are forced closer to zero.
Observations for  sparsity hyperparmeter set to 1 can be found at Observations/AutoEncoder/2015-05-31 23:27:12
Observarions for sparsity hyperparmeter set to .01 can be found at Observations/AutoEncoder/2015-05-31 23:33:01


Problem 25
-----------
Each tile in autoencoderfilter.png represent the pattern or stucture the hidden unit has learnt. As the sparsity parameter is increased more hidden units have activation closer to zero



Problem 26
-----------

MNIST dataset itself is sparse because each image contains only a digit and most of the area in the image is filled with zeros. 

Sparse encoding of MNIST datasets creates a new representation of the MNIST data as a linear combination of basis vectors. Since its a sparse representation most of the coefficients of basis vectors will be close to zero



Usage
=====
__Before starting copy 'cifar-10-batches-py'(unzipped) and mnist.pkl.gz to data folder__

PCA
------------------

Implementation of PCA for mnist and cifar dataset

```
usage: main_PCA.py [-h] [-d D]

optional arguments:
-h, --help  show this help message and exit
-d D        data set to be used 1. 'mnist', 2. 'cifar' (default: mnist)

```

Sparse Auto Encoder
--------------------
Implementation of the sparse encoder that supports different reconstruction cost functions and sparsity cost functions.

The following sparsity costs are supported:

1. KL Divergence

2. L1 Penalty

The following reconstruction costs are supported:

1. Squared Error

2. Cross Entropy

```
usage: main_sparse_autoencoder.py [-h] [-hu HU] [-t T] [-b BATCH_SIZE]
[-l LEARNING_RATE] [-r R] [-sc SC] [-sl SL]

optional arguments:
-h, --help            show this help message and exit
-hu HU                number of hidden units (default: 500)
-t T                  maximum iterations (default: 20)
-b BATCH_SIZE, --batch_size BATCH_SIZE
size of batch (default: 600)
-l LEARNING_RATE, --learning_rate LEARNING_RATE
learning rate (default: 0.01)
-r R                  reconstruction cost function 1. 'cross_entropy' 2.
'sqr' - squared error function (default:
cross_entropy)
-sc SC                sparsity cost function 1. 'kl' - KL Divergence 2. 'l1'
- L1 penalty (default: kl)
-sl SL, --sl SL       sparsity regularization constant (default: 0.01)

```


Folder Structure
=================

__/src__             - Contains all the source code

__/Observations__ - Each run creates a floder with a new timestamp and contains observations and plots generated by the run

__/data__            - Contains the folder 'cifar-10-batches-py' and mnist.pkl.gz

All python root executables start with the name "main_*".py

