Assignment 3
=============

Problem 14
-----------
Neural network is implemented in the file - NeuralNetwork.py
Gradient Descent with mini batches is implemented in the file - gradient_descent_trainer.py
RMSPRop is implemented in the file - rms_prop_trainer.py

Early stopping condition is used to stop training when the validation error 

L1, L2 regularization are implemented as part of the cost function in training algorithm
Dropout implemented in the NeuralNetwork drops the weights according to dropout_rate

Problem 15
-----------
Folder __Observations/2015-05-30 19:35:24__ contains the error curves generated with tanh activation and 300 hidden units


Problem 16
----------
The choice of weight initialization is dependent on the activation function used because we want the gradients to be maximum in the begining

The graph Observations/activation_comparison.png shows a comparison of training errors over each iteration with different activation functions

Problem 17
----------
Each successful execution creates the plot error.png in the directory named with the current timestamp.

Problem 18
-----------

Each successful execution creates the plot repFields.png in the directory named with the current timestamp.

Problem 19
-----------
Custom implemented RMSProp trainer can acheive a test error of ~2.2 %. The sample run can be found in the folder Observations/2015-05-31 15:13:19. 


Usage
======

Neural Network
---------------
Implementation of neural network with regularization featues like dropout, l1,l2 regularization. The hidden layer of the can support multiple functions. The number of neurons can also be configured

```
usage: main_LR.py [-h] [-a ACTIVATION] [-hu HU] [-d DROPOUT_RATE]
{gd,rmsprop} ...

positional arguments:
{gd,rmsprop}
gd                  Gradient Descent
rmsprop             RMSProp algo

optional arguments:
-h, --help            show this help message and exit
-a ACTIVATION, --activation ACTIVATION
activation to be used in the hidden layer 1. tanh 2.
sigmoid 3. relu (default: tanh)
-hu HU                number of neurons in the hidden layer (default: 300)
-d DROPOUT_RATE, --dropout_rate DROPOUT_RATE
Regularization Drop out rate (default: 0.0)

```

### Gradient Descent

To train the network using custom implemented gradient descent method
```
usage: main_LR.py gd [-h] [-t T] [-b BATCH_SIZE] [-l LEARNING_RATE] [-l1 L1]
[-l2 L2]

optional arguments:
-h, --help            show this help message and exit
-t T                  maximum iterations
-b BATCH_SIZE, --batch_size BATCH_SIZE
size of batch
-l LEARNING_RATE, --learning_rate LEARNING_RATE
learning rate
-l1 L1, --l1 L1       L1 Regularization constant
-l2 L2, --l2 L2       L2 Regularization constant
```

### RMSProp

To train the network using custom implemented rmsprop algorithm

```
usage: main_LR.py rmsprop [-h] [-t T] [-b BATCH_SIZE] [-l LEARNING_RATE]
[-l1 L1] [-l2 L2] [-d DECAY]

optional arguments:
-h, --help            show this help message and exit
-t T                  maximum iterations
-b BATCH_SIZE, --batch_size BATCH_SIZE
size of batch
-l LEARNING_RATE, --learning_rate LEARNING_RATE
learning rate
-l1 L1, --l1 L1       L1 Regularization constant
-l2 L2, --l2 L2       L2 Regularization constant
-d DECAY, --decay DECAY
Decay parameter for the moving average.
```



Folder Structure
=================

__/src__             - Contains all the source code

__/Observations__ - Each run creates a floder with a new timestamp and contains observations and plots generated by the run

__/data__            - Contains the data

All python root executables start with the name "main_*".py

