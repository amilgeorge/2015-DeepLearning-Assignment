Assignment 2
=============

Problem 8
-------------
Logistic regression is implemented in the file __LogisticRegression.py__. It supports features like L1, and L2 regularization.
Training algorithms are defined in separate files.
Standandard Gradient Descent is implemented in the file __gradient_descent_trainer.py__

Stopping criterion tested:

1. When validation error increases beyond a certain threshold margin compared to the best validation error training stops. 
Without the threshold it was observed that training stops abruptly because of local minima.

2. When validation error on the validation set is greater than the error on the test set


Problem 9
-------------
Logistic Regression with custom implemented gradient descent trainer achieves about 6.9% error. Plots and data generated are present in the __./Observations__ directory

Problem 10
-------------

Following algorithms were implemented using the climin library:

__Gradient Descent__ - climin_trainer.py

__RMSProp__          - climin_rmsprop_trainer.py

__Climin GD__        - climin_trainer.py


Problem 11
------------

Receptive field images are generated by the training algorithms after fixed iterations. They are numbered repFieldsXXX.png.
The final weight matrix is visualized in repFields.png.

Problem 12
------------

Error curves are saved in the Observation directory.(Observations/2015-05-28 05:15:22/error.png)

Problem 13
------------

GD method acheives a training error of ~7.09 % with certain parameter settings and a stopping criteria that allowed to continue training till the validation error increases beyond a certain threshold margin compared to the best validation error. Please refer to the folder Observations/2015-05-28 05:15:22.

Bonus Question
---------------
It is a bad practice to force the error rate to become too low because it might result in overfitting. We want our ML algorithms to perform well in real world conditions and not only on the training data.

We use three data sets during training:

1. Train Set
2. Validation set
3. Test Set

The test set is not used during the training phase inorder to ensure that we have approximated the model correctly during our training phase.
If we try to force the error rate on the test set to be low it defeats the purpose of the test set. Ideally we should look at the test error only after we have trained the model and picked the parameters

Usage
=====


__Before starting copy mnist.pkl.gz to data folder__

Logistic Regression
------------------

Logistic regression supports various optimization algorithms. The file main_LR.py accepts the optimization algo as a compulsory parameter
The parameters for the optimization algo can be passed as optional arguments.
```
usage: main_LR.py [-h] {gd,rmsprop,climin_gd} ...

positional arguments:
{gd,rmsprop,climin_gd}

optional arguments:
-h, --help            show this help message and exit
```

### GD

To do logistic regression using the custom implemented gradient descent training algorithm

```
usage: main_LR.py gd [-h] [-t T] [-b BATCH_SIZE] [-l LEARNING_RATE] [-l1 L1]
[-l2 L2]

optional arguments:
-h, --help            show this help message and exit
-t T                  maximum iterations
-b BATCH_SIZE, --batch_size BATCH_SIZE
size of batch
-l LEARNING_RATE, --learning_rate LEARNING_RATE
learning rate
-l1 L1, --l1 L1       L1 Regularization constant
-l2 L2, --l2 L2       L2 Regularization constant
```

### RMSProp 

To do logistic regression using the rmsprop training algorithm

```
usage: main_LR.py rmsprop [-h] [-t T] [-b BATCH_SIZE] [-l LEARNING_RATE]
[-l1 L1] [-l2 L2] [-d DECAY] [-m MOMENTUM]

optional arguments:
-h, --help            show this help message and exit
-t T                  maximum iterations
-b BATCH_SIZE, --batch_size BATCH_SIZE
size of batch
-l LEARNING_RATE, --learning_rate LEARNING_RATE
learning rate
-l1 L1, --l1 L1       L1 Regularization constant
-l2 L2, --l2 L2       L2 Regularization constant
-d DECAY, --decay DECAY
Decay parameter for the moving average.
-m MOMENTUM, --momentum MOMENTUM
Momentum to use during optimization.
```
### Climin GD

To do logistic regression using the climin gradient descent training algorithm

```
usage: main_LR.py climin_gd [-h] [-t T] [-b BATCH_SIZE] [-l LEARNING_RATE]
[-l1 L1] [-l2 L2]

optional arguments:
-h, --help            show this help message and exit
-t T                  maximum iterations
-b BATCH_SIZE, --batch_size BATCH_SIZE
size of batch
-l LEARNING_RATE, --learning_rate LEARNING_RATE
learning rate
-l1 L1, --l1 L1       L1 Regularization constant
-l2 L2, --l2 L2       L2 Regularization constant
```




Folder Structure
=================

__/src__             - Contains all the source code

__/Observations__ - Each run creates a floder with a new timestamp and contains observations and plots generated by the run

__/data__            - Contains the data 'mnist.pkl.gz'

All python root executables start with the name "main_*".py

